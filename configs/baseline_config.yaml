# Baseline Bi-LSTM Training Configuration

# Data settings
data:
  splits_dir: data/processed/splits
  audio_feature_dir: data/processed/audio_features
  parameter_dir: data/processed/parameters
  audio_feature_type: mel  # 'mel' or 'mfcc'
  parameter_type: geometric  # 'geometric' or 'pca'
  sequence_length: null  # null = use full utterances, or set fixed length (e.g., 500)

# Model settings
model:
  name: BaselineLSTM
  input_dim: 80  # 80 for mel-spectrogram, 13 for MFCC
  hidden_dim: 128
  num_layers: 2
  output_dim: 14  # 14 for geometric, 10 for PCA
  dropout: 0.3
  learning_rate: 0.001

# Training settings
training:
  batch_size: 8  # Small batch size for variable-length sequences
  num_epochs: 50
  num_workers: 0  # Single-threaded to avoid worker crashes
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  precision: 32  # 16 for mixed precision, 32 for full precision

# Optimization
optimization:
  optimizer: Adam
  learning_rate: 0.001
  weight_decay: 0.0001
  lr_scheduler: ReduceLROnPlateau
  lr_scheduler_params:
    mode: min
    factor: 0.5
    patience: 5
    verbose: true

# Callbacks
callbacks:
  early_stopping:
    monitor: val_loss
    patience: 10
    mode: min
    verbose: true
  model_checkpoint:
    monitor: val_loss
    mode: min
    save_top_k: 3
    save_last: true
    filename: "baseline-{epoch:02d}-{val_loss:.4f}"

# Logging
logging:
  experiment_name: baseline_lstm_v1
  log_dir: logs/training
  save_dir: models/baseline_lstm
  log_every_n_steps: 10

# Evaluation
evaluation:
  metrics:
    - rmse
    - mae
    - pearson_correlation
  visualize_predictions: true
  num_samples_to_visualize: 5

# Reproducibility
seed: 42

# Hardware
device: cpu  # 'cpu' or 'cuda'
