{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": "# Project Sullivan - Google Drive Training (500GB Dataset)\n\n**Phase 2: Large-Scale Training with Session Persistence**\n\nì´ ë…¸íŠ¸ë¶ì€ Google Driveì— ì €ì¥ëœ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹(500GB)ì„ í™œìš©í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.\n\n---\n\n## ğŸ”‘ ì£¼ìš” ê¸°ëŠ¥\n\n1. **Google Drive ì§ì ‘ ë§ˆìš´íŠ¸**: 500GB ë°ì´í„°ì…‹ ì ‘ê·¼\n2. **ì„¸ì…˜ ìœ ì§€ (Keep-Alive)**: 90ë¶„ ë¹„í™œì„± ì¢…ë£Œ ë°©ì§€\n3. **ìë™ ì²´í¬í¬ì¸íŒ…**: ë§¤ ì—í¬í¬ë§ˆë‹¤ Google Driveì— ì €ì¥\n4. **í•™ìŠµ ì¬ê°œ**: ì„¸ì…˜ ì¢…ë£Œ í›„ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì´ì–´ì„œ í•™ìŠµ\n5. **ğŸ†• SSH ì›ê²© ì ‘ì†**: ë¡œì»¬ í„°ë¯¸ë„ì—ì„œ Colab ì§ì ‘ ì œì–´\n\n---\n\n## âš ï¸ ì‚¬ì „ ìš”êµ¬ì‚¬í•­\n\n1. Google Driveì— ë°ì´í„°ì…‹ ì—…ë¡œë“œ ì™„ë£Œ (`Sullivan_Dataset/` í´ë”)\n2. GPU ëŸ°íƒ€ì„ ì„¤ì •: Runtime â†’ Change runtime type â†’ GPU (T4)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "config"
            },
            "source": "## âš™ï¸ Configuration"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "config_vars"
            },
            "outputs": [],
            "source": "# ============================================\n# Training Configuration\n# ============================================\n\n# GitHub Repository\nGITHUB_REPO = 'faransansj/Project_Sullivan'\nBRANCH = 'main'\n\n# Google Drive paths\nGDRIVE_BASE = '/content/drive/MyDrive'\nDATA_DIR = f'{GDRIVE_BASE}/Sullivan_Dataset'\nCHECKPOINT_DIR = f'{GDRIVE_BASE}/Sullivan_Checkpoints'\nLOG_DIR = f'{GDRIVE_BASE}/Sullivan_Logs'\n\n# Training settings\nCONFIG_FILE = 'configs/colab_gdrive_config.yaml'\nRESUME_TRAINING = True  # True: ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ, False: ìƒˆë¡œ ì‹œì‘\nQUICK_TEST = False      # True: 10 ì—í¬í¬ í…ŒìŠ¤íŠ¸, False: ì „ì²´ í•™ìŠµ\n\n# SSH settings\nSSH_PASSWORD = 'sullivan2025'  # SSH ì ‘ì† ë¹„ë°€ë²ˆí˜¸"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## ğŸ”§ Setup Environment"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "mount_drive"
            },
            "outputs": [],
            "source": "# 1. Mount Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nimport os\n\n# Verify data directory\nif os.path.exists(DATA_DIR):\n    print(f'âœ… Data directory found: {DATA_DIR}')\n    !ls -lh {DATA_DIR}\nelse:\n    print(f'âŒ Data directory not found: {DATA_DIR}')\n    print('Please upload your dataset to Google Drive first.')\n\n# Create checkpoint and log directories\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nos.makedirs(LOG_DIR, exist_ok=True)\nprint(f'\\nâœ… Checkpoint dir: {CHECKPOINT_DIR}')\nprint(f'âœ… Log dir: {LOG_DIR}')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "check_gpu"
            },
            "outputs": [],
            "source": "# 2. Check GPU availability\nimport torch\nprint(f'PyTorch version: {torch.__version__}')\nprint(f'CUDA available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    print(f'GPU device: {torch.cuda.get_device_name(0)}')\n    print(f'GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\nelse:\n    print('âš ï¸ GPU not available! Please change runtime: Runtime â†’ Change runtime type â†’ GPU')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "clone_repo"
            },
            "outputs": [],
            "source": "# 3. Clone/Update GitHub repository\nimport os\n\nif not os.path.exists('Project_Sullivan'):\n    !git clone https://github.com/{GITHUB_REPO}.git\n    %cd Project_Sullivan\nelse:\n    %cd Project_Sullivan\n    !git pull origin {BRANCH}\n\nprint('\\nâœ… Repository ready!')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": "# 4. Install dependencies\nprint('ğŸ“¦ Installing dependencies...\\n')\n!pip install -q torch torchvision torchaudio\n!pip install -q pytorch-lightning tensorboard\n!pip install -q librosa soundfile\n!pip install -q numpy scipy matplotlib seaborn\n!pip install -q pyyaml tqdm\n\nprint('\\nâœ… Dependencies installed!')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## ğŸ”Œ SSH Remote Access (Optional)\n\në¡œì»¬ í„°ë¯¸ë„ì—ì„œ Colabì— SSHë¡œ ì ‘ì†í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. AI ì–´ì‹œìŠ¤í„´íŠ¸ê°€ ì‹¤ì‹œê°„ìœ¼ë¡œ í•™ìŠµì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "ssh_setup"
            },
            "outputs": [],
            "source": "# SSH Setup via Cloudflare Tunnel\n# ì´ ì…€ì„ ì‹¤í–‰í•˜ë©´ ë¡œì»¬ì—ì„œ SSHë¡œ ì ‘ì†í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n!pip install colab-ssh --quiet\n\nfrom colab_ssh import launch_ssh_cloudflared\n\nprint('='*60)\nprint('ğŸ”Œ SSH í„°ë„ ì„¤ì • ì¤‘...')\nprint('='*60)\n\n# Launch SSH with cloudflared tunnel\nlaunch_ssh_cloudflared(password=SSH_PASSWORD)\n\nprint('\\n' + '='*60)\nprint('ğŸ“‹ ë¡œì»¬ì—ì„œ ì—°ê²°í•˜ë ¤ë©´:')\nprint('='*60)\nprint('1. ìœ„ì— ì¶œë ¥ëœ Hostnameì„ ë³µì‚¬í•˜ì„¸ìš”')\nprint('2. ë¡œì»¬ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰:')\nprint('   ./scripts/colab_connect.sh save')\nprint('   (ìœ„ì˜ Hostname ì…ë ¥)')\nprint('3. SSH ì ‘ì†:')\nprint('   ./scripts/colab_connect.sh connect')\nprint(f'4. ë¹„ë°€ë²ˆí˜¸: {SSH_PASSWORD}')\nprint('='*60)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## ğŸ›¡ï¸ Session Keep-Alive\n\nì„¸ì…˜ì´ ë¹„í™œì„±ìœ¼ë¡œ ì¸í•´ ì¢…ë£Œë˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤. ì´ ì…€ì„ í•™ìŠµ ì „ì— ì‹¤í–‰í•˜ì„¸ìš”."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "keepalive"
            },
            "outputs": [],
            "source": "# Keep-Alive: ì„¸ì…˜ ìœ ì§€ë¥¼ ìœ„í•œ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ\nimport threading\nimport time\nfrom IPython.display import display, Javascript\n\ndef keep_alive_thread():\n    \"\"\"Background thread to keep session alive.\"\"\"\n    while True:\n        time.sleep(60)  # 1ë¶„ë§ˆë‹¤\n        print('.', end='', flush=True)  # í™œë™ í‘œì‹œ\n\n# Start keep-alive thread\nkeepalive = threading.Thread(target=keep_alive_thread, daemon=True)\nkeepalive.start()\nprint('ğŸ›¡ï¸ Keep-alive thread started. Session will stay active.')\n\n# JavaScript keep-alive (additional safety)\ndisplay(Javascript('''\nfunction KeepAlive() {\n    console.log(\"Keep-alive ping\");\n}\nsetInterval(KeepAlive, 60000);\nconsole.log(\"JavaScript keep-alive started\");\n'''))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## ğŸ’¾ Checkpoint Management\n\nì´ì „ ì²´í¬í¬ì¸íŠ¸ë¥¼ í™•ì¸í•˜ê³ , í•™ìŠµ ì¬ê°œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "check_checkpoints"
            },
            "outputs": [],
            "source": "# Check existing checkpoints\nimport glob\nimport os\n\ncheckpoints = glob.glob(f'{CHECKPOINT_DIR}/*.ckpt')\n\nif checkpoints:\n    checkpoints.sort(key=os.path.getctime, reverse=True)\n    print(f'ğŸ’¾ Found {len(checkpoints)} checkpoint(s):')\n    for ckpt in checkpoints[:5]:\n        size = os.path.getsize(ckpt) / 1e6\n        print(f'   - {os.path.basename(ckpt)} ({size:.1f} MB)')\n    \n    LATEST_CHECKPOINT = checkpoints[0]\n    print(f'\\nğŸ”„ Latest checkpoint: {os.path.basename(LATEST_CHECKPOINT)}')\n    \n    if RESUME_TRAINING:\n        print('âœ… Training will RESUME from latest checkpoint.')\n    else:\n        print('âš ï¸ RESUME_TRAINING=False: Training will start fresh (ignoring checkpoints).')\nelse:\n    print('ğŸ“­ No checkpoints found. Training will start fresh.')\n    LATEST_CHECKPOINT = None"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## ğŸ‹ï¸ Model Training"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "train_model"
            },
            "outputs": [],
            "source": "# Start training with checkpoint resume support\nimport subprocess\nimport sys\n\nprint(f'ğŸš€ Starting Transformer training...')\nprint(f'   Config: {CONFIG_FILE}')\nprint(f'   Data: {DATA_DIR}')\nprint(f'   Checkpoints: {CHECKPOINT_DIR}')\nprint(f'   Resume: {RESUME_TRAINING}')\nprint(f'   Quick test: {QUICK_TEST}')\nprint()\n\n# Build command\ncmd = [\n    sys.executable, 'scripts/train_transformer.py',\n    '--config', CONFIG_FILE,\n    '--gpus', '1',\n    '--data-dir', DATA_DIR,\n    '--checkpoint-dir', CHECKPOINT_DIR,\n    '--log-dir', LOG_DIR,\n]\n\nif RESUME_TRAINING and LATEST_CHECKPOINT:\n    cmd.extend(['--resume-from', LATEST_CHECKPOINT])\n\nif QUICK_TEST:\n    cmd.extend(['--max-epochs', '10'])\n\n# Run training\nprint(f'Command: {\" \".join(cmd)}')\nprint('=' * 60)\n\nprocess = subprocess.Popen(\n    cmd,\n    stdout=subprocess.PIPE,\n    stderr=subprocess.STDOUT,\n    universal_newlines=True,\n    bufsize=1\n)\n\nfor line in iter(process.stdout.readline, ''):\n    print(line, end='')\n\nprocess.wait()\n\nif process.returncode == 0:\n    print('\\nâœ… Training completed successfully!')\nelse:\n    print(f'\\nâŒ Training failed with return code: {process.returncode}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## ğŸ“Š Monitor Training with TensorBoard"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "tensorboard"
            },
            "outputs": [],
            "source": "# Launch TensorBoard\n%load_ext tensorboard\n%tensorboard --logdir {LOG_DIR}"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## ğŸ“ˆ View Training Results"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "show_results"
            },
            "outputs": [],
            "source": "# Check final results\nimport glob\nimport os\nimport pandas as pd\n\n# Find latest checkpoint\ncheckpoints = glob.glob(f'{CHECKPOINT_DIR}/*.ckpt')\nif checkpoints:\n    checkpoints.sort(key=os.path.getctime, reverse=True)\n    print(f'ğŸ’¾ Final checkpoints ({len(checkpoints)} total):')\n    for ckpt in checkpoints[:3]:\n        size = os.path.getsize(ckpt) / 1e6\n        print(f'   - {os.path.basename(ckpt)} ({size:.1f} MB)')\n\n# Check metrics if available\nmetrics_files = glob.glob(f'{LOG_DIR}/**/metrics.csv', recursive=True)\nif metrics_files:\n    latest_metrics = max(metrics_files, key=os.path.getctime)\n    print(f'\\nğŸ“Š Training Metrics: {latest_metrics}')\n    df = pd.read_csv(latest_metrics)\n    print(df.tail(10))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "---\n\n## ğŸ“ Notes\n\n### Session Persistence\n- **Keep-Alive**: ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œê°€ ì„¸ì…˜ ìœ ì§€\n- **Auto-Checkpoint**: ë§¤ ì—í¬í¬ë§ˆë‹¤ Google Driveì— ì €ì¥\n- **Resume**: ì„¸ì…˜ ì¢…ë£Œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ìë™ìœ¼ë¡œ ì´ì–´ì„œ í•™ìŠµ\n\n### SSH Remote Access\n- **SSH Setup ì…€ ì‹¤í–‰ í›„** ë¡œì»¬ì—ì„œ `./scripts/colab_connect.sh save` ë¡œ ì—°ê²°ì •ë³´ ì €ì¥\n- `./scripts/colab_connect.sh connect` ë¡œ SSH ì ‘ì†\n- `./scripts/colab_connect.sh run 'nvidia-smi'` ë¡œ ì›ê²© ëª…ë ¹ ì‹¤í–‰\n\n### Troubleshooting\n\n**GPU OOM:**\n- `configs/colab_gdrive_config.yaml`ì—ì„œ `batch_size` ì¤„ì´ê¸°\n- `precision: 16` (Mixed precision) í™œì„±í™” í™•ì¸\n\n**ì„¸ì…˜ ì¢…ë£Œë¨:**\n- ì´ ë…¸íŠ¸ë¶ ë‹¤ì‹œ ì‹¤í–‰ (RESUME_TRAINING=True)\n- ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ìë™ ì¬ê°œ\n\n**ë°ì´í„° ë¡œë”© ëŠë¦¼:**\n- Streaming DataLoader ì‚¬ìš© í™•ì¸\n- `num_workers` ì¡°ì •\n\n---\n\n**Last Updated**: 2025-12-23\n**Project**: Sullivan - Acoustic-to-Articulatory Inversion\n**Setup**: Google Drive 500GB + Colab Free Tier + SSH Remote Access"
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}