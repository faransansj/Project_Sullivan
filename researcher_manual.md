# Project Sullivan - Researcher Manual
## ìŒì„± ê¸°ë°˜ ë°œìŒ ê¸°ê´€ íŒŒë¼ë¯¸í„° ì¶”ë¡  (Acoustic-to-Articulatory Inversion)

**Version:** 1.1
**Last Updated:** 2025-11-25
**Project Type:** Long-term Multimodal Research

---

## ğŸ“‹ ëª©ì°¨ (Table of Contents)

1. [í”„ë¡œì íŠ¸ ê°œìš”](#1-í”„ë¡œì íŠ¸-ê°œìš”)
2. [ì—°êµ¬ ìˆ˜í–‰ ë§¤ë‰´ì–¼](#2-ì—°êµ¬-ìˆ˜í–‰-ë§¤ë‰´ì–¼)
3. [ìƒì„¸ ì—°êµ¬ ê³„íš](#3-ìƒì„¸-ì—°êµ¬-ê³„íš)
4. [ì„ í–‰ ì—°êµ¬ ë¶„ì„](#4-ì„ í–‰-ì—°êµ¬-ë¶„ì„)
5. [í‰ê°€ ì§€í‘œ](#5-í‰ê°€-ì§€í‘œ)
6. [ì´ˆê¸° ì„¤ì • ë° ë‹¹ë©´ ê³¼ì œ](#6-ì´ˆê¸°-ì„¤ì •-ë°-ë‹¹ë©´-ê³¼ì œ)
7. [ë¶€ë¡](#7-ë¶€ë¡)

---

## 1. í”„ë¡œì íŠ¸ ê°œìš”

### 1.1. ì—°êµ¬ ëª©í‘œ

#### ëª©í‘œ 1 (Primary Goal) - í•µì‹¬ ì—°êµ¬ ëª©í‘œ â­

**ìŒì„± ì‹ í˜¸ë¡œë¶€í„° ë°œìŒ ê¸°ê´€ íŒŒë¼ë¯¸í„° ì¶”ë¡  (Acoustic-to-Articulatory Inversion)**

ìŒì„±(ì˜¤ë””ì˜¤) ì‹ í˜¸ë§Œì„ ì…ë ¥ë°›ì•„ **ë°œìŒ ê¸°ê´€ì˜ ìœ„ì¹˜, í˜•íƒœ, ì›€ì§ì„ì„ ë‚˜íƒ€ë‚´ëŠ” ì €ì°¨ì› íŒŒë¼ë¯¸í„°(Articulatory Parameters)**ë¥¼ ì •í™•í•˜ê²Œ ì¶”ë¡ í•˜ëŠ” AI ëª¨ë¸ì„ ê°œë°œí•˜ëŠ” ê²ƒì´ ë³¸ ì—°êµ¬ì˜ **í•µì‹¬ ëª©í‘œ**ì…ë‹ˆë‹¤.

**Input**: ìŒì„± íŒŒí˜• (Audio Waveform)
**Output**: ë°œìŒ ê¸°ê´€ íŒŒë¼ë¯¸í„° (í˜€ ìœ„ì¹˜, í„± ê°œë°©ë„, ì…ìˆ  ëª¨ì–‘ ë“±)

ì´ëŠ” ìŒì„±í•™, ì–¸ì–´ë³‘ë¦¬í•™, ìŒì„± í•©ì„±, ì¸ê°„-ì»´í“¨í„° ìƒí˜¸ì‘ìš© ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë  ìˆ˜ ìˆëŠ” ê¸°ì´ˆ ê¸°ìˆ ì…ë‹ˆë‹¤.

#### ëª©í‘œ 2 (Secondary Goal) - í–¥í›„ í™•ì¥ ì—°êµ¬ ëª©í‘œ

**ë””ì§€í„¸ íŠ¸ìœˆ êµ¬ì¶• ë° ìŒí–¥ ì¬í•©ì„±**

ëª©í‘œ 1ì—ì„œ ì¶”ë¡ ëœ ë°œìŒ ê¸°ê´€ íŒŒë¼ë¯¸í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 3D ë°œìŒ ê¸°ê´€ ëª¨ë¸(Digital Twin)ì„ ìƒì„±í•˜ê³ , ì´ë¥¼ í†µí•´ ë¬¼ë¦¬ ê¸°ë°˜ ìŒí–¥ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ì†Œë¦¬ë¥¼ ì¬í•©ì„±í•˜ëŠ” ì—°êµ¬ì…ë‹ˆë‹¤. ì´ëŠ” **ëª©í‘œ 1ì´ ì„±ê³µì ìœ¼ë¡œ ë‹¬ì„±ëœ í›„** ì§„í–‰ë  í™•ì¥ ì—°êµ¬ì…ë‹ˆë‹¤.

### 1.2. í•µì‹¬ ë°ì´í„°ì…‹

- **USC-TIMIT / USC Speech MRI Dataset**
  - Real-time MRI (rtMRI) ì˜ìƒ
  - ë™ê¸°í™”ëœ ì˜¤ë””ì˜¤ ë°ì´í„°
  - ë°œìŒ ì¤‘ ì„±ë„(Vocal Tract)ì˜ ë™ì  ì›€ì§ì„ í¬ì°©

### 1.3. ì—°êµ¬ì˜ í•µì‹¬ ê°€ì¹˜

- **ì¬í˜„ì„± (Reproducibility)**: ëª¨ë“  ì‹¤í—˜ì€ ì¶”ì  ê°€ëŠ¥í•˜ê³  ì¬í˜„ ê°€ëŠ¥í•´ì•¼ í•¨
- **í˜‘ì—…ì„± (Collaboration)**: íŒ€ì› ëˆ„êµ¬ë‚˜ ì´ì „ ì‘ì—…ì„ ì´í•´í•˜ê³  ì´ì–´ê°ˆ ìˆ˜ ìˆì–´ì•¼ í•¨
- **ê³¼í•™ì  ì—„ë°€ì„± (Scientific Rigor)**: ì •ëŸ‰ì  ì§€í‘œë¡œ ëª¨ë“  ì§„ì²™ì„ ì¸¡ì •

---

## 2. ì—°êµ¬ ìˆ˜í–‰ ë§¤ë‰´ì–¼

### 2.1. ì—°êµ¬ ë¡œê·¸(Research Log) ì‘ì„± ì›ì¹™

ëª¨ë“  ì—°êµ¬ì›ì€ ì‘ì—… ì¢…ë£Œ ì‹œ ë˜ëŠ” ì£¼ìš” ë§ˆì¼ìŠ¤í†¤ ë‹¬ì„± ì‹œ **ì¦‰ì‹œ** ì—°êµ¬ ë¡œê·¸ë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

#### 2.1.1. ì‘ì„± ì‹œì 
- ì¼ì¼ ì‘ì—… ì¢…ë£Œ ì‹œ
- ì£¼ìš” ë§ˆì¼ìŠ¤í†¤ ë‹¬ì„± ì‹œ
- ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ ë°œìƒ ì‹œ
- ë‹¤ë¥¸ íŒ€ì›ì—ê²Œ ì‘ì—…ì„ ì¸ê³„í•  ë•Œ

#### 2.1.2. í•„ìˆ˜ í¬í•¨ í•­ëª©

```markdown
## Research Log Entry

**Date/Time**: YYYY-MM-DD HH:MM (KST)
**Researcher**: [ì´ë¦„]
**Commit Hash**: [Git ì»¤ë°‹ í•´ì‹œ] (ì½”ë“œ ë³€ê²½ ì‹œ)
**Experiment ID**: EXP-YYYYMMDD-NN

### Parameters
- [ë³€ê²½ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë˜ëŠ” ì„¤ì •ê°’]
- [ì˜ˆ: learning_rate = 0.001, batch_size = 32]

### Objective
- [ì´ë²ˆ ì‹¤í—˜/ì‘ì—…ì˜ ëª©í‘œ]

### Method
- [ì‚¬ìš©í•œ ë°©ë²•ë¡  ê°„ë‹¨ ìš”ì•½]
- [ì°¸ê³ í•œ ë…¼ë¬¸ ë˜ëŠ” ì½”ë“œ ë§í¬]

### Results
- **Status**: [Success/Failed/Partial]
- **Quantitative**: [ì •ëŸ‰ì  ì§€í‘œ - Loss, Accuracy, etc.]
- **Qualitative**: [ì •ì„±ì  ê´€ì°° ì‚¬í•­]
- **Output Files**: [ìƒì„±ëœ íŒŒì¼ ê²½ë¡œ]

### Analysis
- [ê²°ê³¼ì— ëŒ€í•œ í•´ì„]
- [ì˜ˆìƒê³¼ì˜ ì°¨ì´ì ]

### Next Steps
- [ ] [ë‹¤ìŒì— ìˆ˜í–‰í•  ì‘ì—… 1]
- [ ] [ë‹¤ìŒì— ìˆ˜í–‰í•  ì‘ì—… 2]

### Notes
- [íŠ¹ì´ì‚¬í•­, ì—ëŸ¬, ì£¼ì˜ì‚¬í•­]
```

#### 2.1.3. ë¡œê·¸ ì €ì¥ ìœ„ì¹˜

```
/logs/
â”œâ”€â”€ YYYY-MM/
â”‚   â”œâ”€â”€ YYYYMMDD_researcher_name.md
â”‚   â””â”€â”€ experiments/
â”‚       â”œâ”€â”€ EXP-YYYYMMDD-01.json
â”‚       â””â”€â”€ EXP-YYYYMMDD-02.json
```

---

### 2.2. ì‘ì—… í• ë‹¹ ë° ì—…ë°ì´íŠ¸ í”„ë¡œì„¸ìŠ¤

#### 2.2.1. ì‘ì—… ì •ì˜ (To-Do)

**ì´ìŠˆ íŠ¸ë˜ì»¤(GitHub Issue/Jira)ì— ë‹¤ìŒ ì •ë³´ë¥¼ ëª…ì‹œí•˜ì—¬ ë“±ë¡:**

```markdown
### Task Title
[ëª…í™•í•œ ì‘ì—… ì œëª©]

### Objective
[ì‘ì—…ì˜ ëª©í‘œì™€ ì˜ì˜]

### Input
- [í•„ìš”í•œ ì…ë ¥ ë°ì´í„°/íŒŒì¼]
- [ì˜ì¡´ì„± ìˆëŠ” ì´ì „ ì‘ì—…]

### Expected Output
- [ì˜ˆìƒë˜ëŠ” ê²°ê³¼ë¬¼]
- [ìƒì„±ë  íŒŒì¼ í˜•ì‹ ë° ìœ„ì¹˜]

### Acceptance Criteria
- [ ] [ì™„ë£Œ ì¡°ê±´ 1]
- [ ] [ì™„ë£Œ ì¡°ê±´ 2]

### Deadline
YYYY-MM-DD

### References
- [ê´€ë ¨ ë…¼ë¬¸ ë§í¬]
- [ì°¸ê³  ì½”ë“œ ë§í¬]

### Assignee
[@username]
```

#### 2.2.2. ì§„í–‰ (In-Progress)

ì‘ì—… ì‹œì‘ ì‹œ:
1. ì´ìŠˆ ìƒíƒœë¥¼ **"In Progress"**ë¡œ ë³€ê²½
2. ì‘ì—… ì‹œì‘ ì‹œê°„ì„ ëŒ“ê¸€ë¡œ ê¸°ë¡
3. ì°¸ê³  ì¤‘ì¸ ë ˆí¼ëŸ°ìŠ¤ë¥¼ ë§í¬ë¡œ ì¶”ê°€

#### 2.2.3. ì™„ë£Œ ë° ì—…ë°ì´íŠ¸ (Done)

ì‘ì—… ì¢…ë£Œ ì‹œ **ê²°ê³¼ ë³´ê³ ì„œ**ë¥¼ ì´ìŠˆ ëŒ“ê¸€ë¡œ ì‘ì„±:

```markdown
## Completion Report

### What was done
- [ìˆ˜í–‰í•œ ì‘ì—… ë‚´ìš©]

### Method
- [ì‚¬ìš©í•œ ì½”ë“œ/ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ]
- [í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ì„¤ëª…]

### Output
- **Files Generated**:
  - `/path/to/output1.npy` - [ì„¤ëª…]
  - `/path/to/output2.png` - [ì„¤ëª…]
- **Metrics**:
  - Accuracy: XX%
  - Loss: XX

### Code Changes
- Commit: [commit hash]
- Files modified:
  - `src/module.py` - [ë³€ê²½ ë‚´ìš©]

### Challenges & Solutions
- [ë°œìƒí•œ ë¬¸ì œì ê³¼ í•´ê²° ë°©ë²•]

### Next Steps for Other Researchers
- [ë‹¤ìŒ ì‘ì—…ìê°€ ì•Œì•„ì•¼ í•  ì‚¬í•­]
```

---

### 2.3. ì½”ë“œ ë° ë°ì´í„° ê´€ë¦¬ ê·œì¹™

#### 2.3.1. ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
Project_Sullivan/
â”œâ”€â”€ data/                      # ë°ì´í„° ì €ì¥ì†Œ (Gitì—ì„œ ì œì™¸)
â”‚   â”œâ”€â”€ raw/                   # ì›ë³¸ ë°ì´í„° (ìˆ˜ì • ê¸ˆì§€)
â”‚   â”‚   â””â”€â”€ usc_speech_mri/
â”‚   â”œâ”€â”€ processed/             # ì „ì²˜ë¦¬ëœ ë°ì´í„°
â”‚   â”‚   â”œâ”€â”€ segmented/
â”‚   â”‚   â””â”€â”€ parameters/
â”‚   â””â”€â”€ experiments/           # ì‹¤í—˜ë³„ ë°ì´í„°
â”‚       â””â”€â”€ EXP-YYYYMMDD-NN/
â”œâ”€â”€ src/                       # ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ preprocessing/         # Phase 1: ì „ì²˜ë¦¬
â”‚   â”œâ”€â”€ modeling/              # Phase 2: ëª¨ë¸ë§
â”‚   â”œâ”€â”€ simulation/            # Phase 3: ì‹œë®¬ë ˆì´ì…˜
â”‚   â””â”€â”€ utils/                 # ê³µí†µ ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ notebooks/                 # Jupyter ë…¸íŠ¸ë¶ (EDA, ì‹œê°í™”)
â”œâ”€â”€ configs/                   # ì„¤ì • íŒŒì¼ (YAML, JSON)
â”œâ”€â”€ logs/                      # ì‹¤í—˜ ë¡œê·¸
â”œâ”€â”€ docs/                      # ë¬¸ì„œ
â”‚   â”œâ”€â”€ researcher_manual.md   # ë³¸ ë¬¸ì„œ
â”‚   â”œâ”€â”€ literature_review.md   # ë…¼ë¬¸ ë¦¬ë·° ì •ë¦¬
â”‚   â””â”€â”€ meeting_notes/         # íšŒì˜ë¡
â”œâ”€â”€ models/                    # í•™ìŠµëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
â”œâ”€â”€ results/                   # ê²°ê³¼ ì´ë¯¸ì§€, ê·¸ë˜í”„
â””â”€â”€ tests/                     # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
```

#### 2.3.2. Git ì»¤ë°‹ ê·œì¹™

```bash
# ì»¤ë°‹ ë©”ì‹œì§€ í˜•ì‹
[TYPE] Brief description

Detailed explanation (optional)

- Related Issue: #123
- Experiment ID: EXP-20251125-01

# TYPE ì¢…ë¥˜:
# [DATA] - ë°ì´í„° ì²˜ë¦¬ ê´€ë ¨
# [MODEL] - ëª¨ë¸ êµ¬í˜„/ìˆ˜ì •
# [EXP] - ì‹¤í—˜ ìˆ˜í–‰
# [FIX] - ë²„ê·¸ ìˆ˜ì •
# [DOCS] - ë¬¸ì„œ ì‘ì„±/ìˆ˜ì •
# [REFACTOR] - ì½”ë“œ ë¦¬íŒ©í† ë§
```

**ì˜ˆì‹œ:**
```bash
git commit -m "[MODEL] Implement Bi-LSTM articulatory parameter predictor

- Added src/modeling/articulation_predictor.py
- Input: Mel-spectrogram (80 bins)
- Output: 10-dim articulatory parameters
- Related Issue: #15
- Experiment ID: EXP-20251125-03"
```

#### 2.3.3. ë¸Œëœì¹˜ ì „ëµ

```
main (ë˜ëŠ” master)
â”œâ”€â”€ develop                    # ê°œë°œ í†µí•© ë¸Œëœì¹˜
â”‚   â”œâ”€â”€ feature/data-preprocessing
â”‚   â”œâ”€â”€ feature/audio-to-param-model
â”‚   â”œâ”€â”€ feature/3d-reconstruction
â”‚   â””â”€â”€ experiment/exp-20251125-01
```

---

### 2.4. ì½”ë“œ ì‘ì„± ê·œì¹™

#### 2.4.1. Python ì½”ë”© ìŠ¤íƒ€ì¼

- **PEP 8** ì¤€ìˆ˜
- í•¨ìˆ˜/í´ë˜ìŠ¤ì— **Docstring** í•„ìˆ˜ ì‘ì„±
- Type Hints ì‚¬ìš© ê¶Œì¥

```python
def extract_mfcc(
    audio_path: str,
    n_mfcc: int = 13,
    sr: int = 16000
) -> np.ndarray:
    """
    Extract MFCC features from audio file.

    Args:
        audio_path: Path to the audio file
        n_mfcc: Number of MFCC coefficients to extract
        sr: Sample rate for audio loading

    Returns:
        MFCC feature matrix of shape (n_mfcc, time_steps)

    Raises:
        FileNotFoundError: If audio file doesn't exist

    Example:
        >>> mfcc = extract_mfcc("data/audio.wav", n_mfcc=13)
        >>> print(mfcc.shape)
        (13, 100)
    """
    # Implementation
    pass
```

#### 2.4.2. ì„¤ì • íŒŒì¼ ì‚¬ìš©

í•˜ë“œì½”ë”© ê¸ˆì§€! ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ì„¤ì • íŒŒì¼ë¡œ ê´€ë¦¬:

```yaml
# configs/phase2_training.yaml
model:
  name: "BiLSTM_Articulation_Predictor"
  architecture:
    input_dim: 80  # Mel-spectrogram bins
    hidden_dim: 256
    num_layers: 3
    output_dim: 10  # Articulatory parameters
    dropout: 0.3

training:
  batch_size: 32
  learning_rate: 0.001
  num_epochs: 100
  optimizer: "Adam"

data:
  train_path: "data/processed/train"
  val_path: "data/processed/val"
  test_path: "data/processed/test"
```

---

## 3. ìƒì„¸ ì—°êµ¬ ê³„íš

### ì—°êµ¬ ìš°ì„ ìˆœìœ„ ë° ë§ˆì¼ìŠ¤í†¤

ë³¸ ì—°êµ¬ëŠ” **2ê°œì˜ í•µì‹¬ Phase(Phase 1-2)**ì™€ **1ê°œì˜ í™•ì¥ Phase(Phase 3)**ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

```
[í•µì‹¬ ì—°êµ¬: ëª©í‘œ 1 ë‹¬ì„±]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Phase 1      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚     Phase 2      â”‚
â”‚  Data Prep &    â”‚         â”‚ Audio-to-Param   â”‚
â”‚ Parameterizationâ”‚         â”‚  Model Training  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²                            â”‚
        â”‚                            â”‚
   rtMRI Data                        â–¼
   + Audio                    Articulatory
                              Parameters âœ“


[í™•ì¥ ì—°êµ¬: ëª©í‘œ 2 (í–¥í›„)]

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Phase 3      â”‚
â”‚  3D Digital Twin â”‚
â”‚  & Synthesis     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**í˜„ì¬ ì§‘ì¤‘ ì‘ì—…: Phase 1 â†’ Phase 2**
- Phase 1-2 ì™„ë£Œê°€ ìµœìš°ì„  ê³¼ì œ
- Phase 3ëŠ” Phase 2ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œëœ í›„ ì°©ìˆ˜

---

### ë§ˆì¼ìŠ¤í†¤ (Milestones)

| Milestone | Target | ì™„ë£Œ ì¡°ê±´ | ì˜ˆìƒ ê¸°ê°„ |
|-----------|--------|----------|-----------|
| **M1: Data Pipeline** | Phase 1 ì™„ë£Œ | MRI-Audio ìŒ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ (Train/Val/Test split) | 4-6ì£¼ |
| **M2: Baseline Model** | Phase 2 ì´ˆê¸° | ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ (RMSE < 0.15) | 2-3ì£¼ |
| **M3: Core Goal Achievement** | Phase 2 ì™„ë£Œ | ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„± (RMSE < 0.10, PCC > 0.70) | 8-12ì£¼ |
| **M4: Digital Twin (Optional)** | Phase 3 ì™„ë£Œ | 3D ëª¨ë¸ ìƒì„± ë° ìŒí–¥ ì¬í•©ì„± | TBD |

---

### Phase 1: ë°ì´í„° ì „ì²˜ë¦¬ ë° íŒŒë¼ë¯¸í„° ì¶”ì¶œ

#### ëª©í‘œ
Raw MRI ì˜ìƒì—ì„œ ë°œìŒ ê¸°ê´€ì˜ 'ì›€ì§ì„'ì„ ëŒ€í‘œí•˜ëŠ” ì €ì°¨ì› íŒŒë¼ë¯¸í„°(Latent Vector) ì¶”ì¶œ

#### 3.1.1. Step 1: ë°ì´í„° ë¡œë”© ë° íƒìƒ‰

**ì‘ì—… ë‚´ìš©:**
- USC Speech MRI ë°ì´í„°ì…‹ ì••ì¶• í•´ì œ ë° êµ¬ì¡° íŒŒì•…
- MRI ì˜ìƒ ë©”íƒ€ë°ì´í„° í™•ì¸ (í•´ìƒë„, fps, í¬ë§·)
- ì˜¤ë””ì˜¤ ë©”íƒ€ë°ì´í„° í™•ì¸ (ìƒ˜í”Œë§ ë ˆì´íŠ¸, ê¸¸ì´, í¬ë§·)
- ì˜¤ë””ì˜¤-ì˜ìƒ ë™ê¸°í™” ìƒíƒœ ê²€ì¦

**ì‚°ì¶œë¬¼:**
```
data/raw/usc_speech_mri/
â”œâ”€â”€ README.md                  # ë°ì´í„°ì…‹ ì„¤ëª…
â”œâ”€â”€ subjects/
â”‚   â”œâ”€â”€ subject_01/
â”‚   â”‚   â”œâ”€â”€ mri_frames/        # MRI ì˜ìƒ í”„ë ˆì„
â”‚   â”‚   â”œâ”€â”€ audio.wav          # ë™ê¸°í™”ëœ ì˜¤ë””ì˜¤
â”‚   â”‚   â””â”€â”€ metadata.json      # ë©”íƒ€ì •ë³´
â”‚   â””â”€â”€ ...

notebooks/01_EDA.ipynb         # íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ë…¸íŠ¸ë¶
docs/data_statistics.md        # ë°ì´í„° í†µê³„ ë³´ê³ ì„œ
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] MRI í”„ë ˆì„ ìˆ˜, í•´ìƒë„, fps í™•ì¸
- [ ] ì˜¤ë””ì˜¤ ìƒ˜í”Œë§ ë ˆì´íŠ¸, ê¸¸ì´ í™•ì¸
- [ ] ì˜¤ë””ì˜¤-MRI ë™ê¸°í™” offset ê³„ì‚°
- [ ] ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ í™•ì¸ (ë…¸ì´ì¦ˆ, ê²°ì¸¡ì¹˜)

---

#### 3.1.2. Step 2: ì „ì²˜ë¦¬ (Denoising & Alignment)

**ì‘ì—… ë‚´ìš©:**
- MRI ì˜ìƒ ë…¸ì´ì¦ˆ ì œê±° (Gaussian/Median filtering)
- ì˜¤ë””ì˜¤ ë…¸ì´ì¦ˆ ì œê±° (Spectral subtraction)
- ì˜¤ë””ì˜¤-ì˜ìƒ ì •ë°€ ì •ë ¬ (Cross-correlation ê¸°ë°˜)
- í”„ë ˆì„ ë ˆì´íŠ¸ í†µì¼ (Interpolation)

**êµ¬í˜„ ì˜ˆì‹œ:**
```python
# src/preprocessing/denoising.py

import cv2
import numpy as np
from scipy import signal

def denoise_mri_frame(frame: np.ndarray, method: str = "gaussian") -> np.ndarray:
    """MRI í”„ë ˆì„ ë…¸ì´ì¦ˆ ì œê±°"""
    if method == "gaussian":
        return cv2.GaussianBlur(frame, (5, 5), 0)
    elif method == "median":
        return cv2.medianBlur(frame, 5)
    else:
        raise ValueError(f"Unknown method: {method}")

def align_audio_mri(
    audio: np.ndarray,
    mri_timestamps: np.ndarray,
    audio_sr: int
) -> tuple[np.ndarray, np.ndarray]:
    """ì˜¤ë””ì˜¤ì™€ MRI íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë ¬"""
    # Cross-correlationìœ¼ë¡œ ìµœì  offset ì°¾ê¸°
    # ...
    return aligned_audio, aligned_timestamps
```

**ì‚°ì¶œë¬¼:**
```
data/processed/aligned/
â”œâ”€â”€ subject_01/
â”‚   â”œâ”€â”€ mri_denoised/          # ë…¸ì´ì¦ˆ ì œê±°ëœ MRI
â”‚   â”œâ”€â”€ audio_clean.wav        # ë…¸ì´ì¦ˆ ì œê±°ëœ ì˜¤ë””ì˜¤
â”‚   â””â”€â”€ alignment_info.json    # ì •ë ¬ ì •ë³´
```

---

#### 3.1.3. Step 3: ROI ë¶„í•  (Segmentation)

**ì‘ì—… ë‚´ìš©:**
- ì„±ë„(Vocal Tract), í˜€, í„±, ì…ìˆ  ì˜ì—­ì„ í”„ë ˆì„ ë‹¨ìœ„ë¡œ ë¶„í• 
- Deep Learning ê¸°ë°˜ Segmentation ëª¨ë¸ í•™ìŠµ ë˜ëŠ” ì‚¬ìš©

**ëª¨ë¸ ì„ íƒì§€:**
- **U-Net**: ì˜ë£Œ ì˜ìƒ ë¶„í• ì˜ í‘œì¤€
- **SegFormer**: Transformer ê¸°ë°˜ ê³ ì„±ëŠ¥ ë¶„í•  ëª¨ë¸
- **Mask R-CNN**: Instance segmentation í•„ìš” ì‹œ

**Ground Truth ìƒì„±:**
- ì†Œìˆ˜ì˜ í”„ë ˆì„ì„ ìˆ˜ë™ìœ¼ë¡œ ë¼ë²¨ë§ (Labelme, CVAT ì‚¬ìš©)
- Semi-supervised learningìœ¼ë¡œ ë¼ë²¨ í™•ì¥

**êµ¬í˜„ ì˜ˆì‹œ:**
```python
# src/preprocessing/segmentation.py

import torch
import torch.nn as nn
from torchvision.models.segmentation import deeplabv3_resnet50

class VocalTractSegmenter:
    def __init__(self, num_classes: int = 5):
        """
        num_classes: ë°°ê²½ + í˜€ + í„± + ì…ìˆ  + ì—°êµ¬ê°œ = 5
        """
        self.model = deeplabv3_resnet50(pretrained=True)
        self.model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=1)

    def segment(self, mri_frame: np.ndarray) -> np.ndarray:
        """MRI í”„ë ˆì„ì„ ì…ë ¥ë°›ì•„ ë¶„í•  ë§ˆìŠ¤í¬ ì¶œë ¥"""
        # Preprocessing
        x = self.preprocess(mri_frame)

        # Inference
        with torch.no_grad():
            output = self.model(x)['out']
            mask = output.argmax(dim=1).cpu().numpy()

        return mask
```

**ì‚°ì¶œë¬¼:**
```
data/processed/segmented/
â”œâ”€â”€ subject_01/
â”‚   â”œâ”€â”€ frame_0001_mask.png
â”‚   â”œâ”€â”€ frame_0002_mask.png
â”‚   â””â”€â”€ ...
models/segmentation/
â””â”€â”€ vocal_tract_segmenter_v1.pth
```

**í‰ê°€ ì§€í‘œ:**
- Dice Coefficient: > 0.85
- IoU (Intersection over Union): > 0.80

---

#### 3.1.4. Step 4: í˜•ìƒ íŒŒë¼ë¯¸í„°í™” (Parameter Extraction)

**ì‘ì—… ë‚´ìš©:**
ë¶„í• ëœ ê³ ì°¨ì› ì´ë¯¸ì§€ë¥¼ ì†Œìˆ˜ì˜ ì œì–´ íŒŒë¼ë¯¸í„°ë¡œ ë³€í™˜

**ë°©ë²•ë¡  1: PCA (Principal Component Analysis)**
```python
# src/preprocessing/parameterization.py

from sklearn.decomposition import PCA
import numpy as np

def extract_pca_parameters(
    segmentation_masks: np.ndarray,  # Shape: (num_frames, H, W)
    n_components: int = 10
) -> np.ndarray:
    """
    PCAë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì°¨ì› íŒŒë¼ë¯¸í„° ì¶”ì¶œ

    Returns:
        parameters: Shape (num_frames, n_components)
    """
    # Flatten masks
    num_frames, H, W = segmentation_masks.shape
    flattened = segmentation_masks.reshape(num_frames, -1)

    # PCA
    pca = PCA(n_components=n_components)
    parameters = pca.fit_transform(flattened)

    print(f"Explained variance ratio: {pca.explained_variance_ratio_}")

    return parameters, pca
```

**ë°©ë²•ë¡  2: Autoencoder**
```python
class ArticulatoryAutoencoder(nn.Module):
    def __init__(self, latent_dim: int = 10):
        super().__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 32, 4, 2, 1),   # (H, W) -> (H/2, W/2)
            nn.ReLU(),
            nn.Conv2d(32, 64, 4, 2, 1),  # -> (H/4, W/4)
            nn.ReLU(),
            nn.Conv2d(64, 128, 4, 2, 1), # -> (H/8, W/8)
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(128 * (H//8) * (W//8), latent_dim)
        )

        # Decoder
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 128 * (H//8) * (W//8)),
            nn.ReLU(),
            nn.Unflatten(1, (128, H//8, W//8)),
            nn.ConvTranspose2d(128, 64, 4, 2, 1),
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 4, 2, 1),
            nn.ReLU(),
            nn.ConvTranspose2d(32, 1, 4, 2, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        z = self.encoder(x)  # Latent parameters
        x_recon = self.decoder(z)
        return x_recon, z
```

**Ground Truth ë°ì´í„°ì…‹ ìƒì„±:**
```
data/processed/parameters/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ subject_01_audio_mfcc.npy      # (num_frames, 13)
â”‚   â”œâ”€â”€ subject_01_parameters.npy      # (num_frames, 10)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ val/
â””â”€â”€ test/
```

**ê° íŒŒì¼ í˜•ì‹:**
- `audio_mfcc.npy`: Mel-Frequency Cepstral Coefficients (MFCC)
- `parameters.npy`: ë°œìŒ ê¸°ê´€ íŒŒë¼ë¯¸í„° (PCA ë˜ëŠ” Autoencoderì˜ latent vector)

---

### Phase 2: ì˜¤ë””ì˜¤-íŒŒë¼ë¯¸í„° ë§¤í•‘ ëª¨ë¸ (í•µì‹¬ ëª©í‘œ)

#### ëª©í‘œ
**ì´ê²ƒì´ ë³¸ ì—°êµ¬ì˜ í•µì‹¬ì…ë‹ˆë‹¤!**
ìŒì„± ì‹ í˜¸ë§Œ ì…ë ¥í–ˆì„ ë•Œ, Phase 1ì—ì„œ ì •ì˜í•œ ë°œìŒ ê¸°ê´€ íŒŒë¼ë¯¸í„°ë¥¼ ì •í™•í•˜ê²Œ ì¶”ë¡ í•˜ëŠ” AI ëª¨ë¸ ê°œë°œ

```
Input: Audio Waveform
   â†“
Mel-Spectrogram Extraction
   â†“
Deep Learning Model (Bi-LSTM / Transformer)
   â†“
Output: Articulatory Parameters (10-dim vector per frame)
```

---

#### 3.2.1. ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„ íƒ

**ì˜µì…˜ 1: Bi-LSTM (Bidirectional LSTM)**
- ì¥ì : ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬ì— ê°•í•¨, ìƒëŒ€ì ìœ¼ë¡œ ê°€ë²¼ì›€
- ë‹¨ì : ì¥ê¸° ì˜ì¡´ì„± ì²˜ë¦¬ì— í•œê³„

```python
# src/modeling/articulation_predictor.py

class BiLSTMArticulationPredictor(nn.Module):
    def __init__(
        self,
        input_dim: int = 80,    # Mel-spectrogram bins
        hidden_dim: int = 256,
        num_layers: int = 3,
        output_dim: int = 10,   # Articulatory parameters
        dropout: float = 0.3
    ):
        super().__init__()

        self.lstm = nn.LSTM(
            input_dim,
            hidden_dim,
            num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout
        )

        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # *2 for bidirectional

    def forward(self, x):
        # x: (batch, time, input_dim)
        lstm_out, _ = self.lstm(x)  # (batch, time, hidden_dim*2)
        output = self.fc(lstm_out)   # (batch, time, output_dim)
        return output
```

**ì˜µì…˜ 2: Conformer (Convolution-augmented Transformer)**
- ì¥ì : SOTA ì„±ëŠ¥, local + global context ëª¨ë‘ í¬ì°©
- ë‹¨ì : ê³„ì‚° ë¹„ìš© ë†’ìŒ

---

#### 3.2.2. ì†ì‹¤ í•¨ìˆ˜ (Loss Function)

**1. MSE Loss (ê¸°ë³¸)**
```python
mse_loss = nn.MSELoss()
loss = mse_loss(predicted_params, target_params)
```

**2. Smoothness Loss (ì‹œê°„ì  ì—°ì†ì„±)**
```python
def smoothness_loss(predictions, alpha=0.1):
    """
    ì—°ì†ëœ í”„ë ˆì„ ê°„ì˜ ê¸‰ê²©í•œ ë³€í™”ë¥¼ íŒ¨ë„í‹°
    """
    diff = predictions[:, 1:, :] - predictions[:, :-1, :]
    smooth_loss = torch.mean(diff ** 2)
    return alpha * smooth_loss
```

**3. Total Loss**
```python
total_loss = mse_loss(pred, target) + smoothness_loss(pred)
```

---

#### 3.2.3. í•™ìŠµ íŒŒì´í”„ë¼ì¸

**ë°ì´í„° ë¡œë”:**
```python
# src/modeling/dataset.py

from torch.utils.data import Dataset, DataLoader
import numpy as np

class ArticulatoryDataset(Dataset):
    def __init__(self, data_dir: str, split: str = "train"):
        self.audio_files = sorted(glob(f"{data_dir}/{split}/*_audio_mfcc.npy"))
        self.param_files = sorted(glob(f"{data_dir}/{split}/*_parameters.npy"))

    def __len__(self):
        return len(self.audio_files)

    def __getitem__(self, idx):
        audio = np.load(self.audio_files[idx])    # (time, 13)
        params = np.load(self.param_files[idx])   # (time, 10)

        # Convert to torch tensors
        audio = torch.FloatTensor(audio)
        params = torch.FloatTensor(params)

        return audio, params

# Usage
train_dataset = ArticulatoryDataset("data/processed/parameters", split="train")
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
```

**í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸:**
```python
# src/modeling/train.py

def train_epoch(model, dataloader, optimizer, criterion, device):
    model.train()
    total_loss = 0

    for audio, params in dataloader:
        audio, params = audio.to(device), params.to(device)

        optimizer.zero_grad()

        # Forward pass
        predictions = model(audio)

        # Loss calculation
        loss = criterion(predictions, params) + smoothness_loss(predictions)

        # Backward pass
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(dataloader)

# Main training loop
def main():
    # Load config
    config = yaml.safe_load(open("configs/phase2_training.yaml"))

    # Initialize model
    model = BiLSTMArticulationPredictor(**config['model']['architecture'])
    model = model.to(device)

    # Optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=config['training']['learning_rate'])

    # Training
    for epoch in range(config['training']['num_epochs']):
        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
        val_loss = validate(model, val_loader, criterion, device)

        print(f"Epoch {epoch}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}")

        # Save checkpoint
        if val_loss < best_val_loss:
            torch.save(model.state_dict(), f"models/phase2_best.pth")
```

---

#### 3.2.4. í‰ê°€

**ì •ëŸ‰ì  ì§€í‘œ:**
```python
# src/modeling/evaluate.py

from sklearn.metrics import mean_squared_error, mean_absolute_error
from scipy.stats import pearsonr

def evaluate_model(model, test_loader, device):
    model.eval()

    all_predictions = []
    all_targets = []

    with torch.no_grad():
        for audio, params in test_loader:
            audio = audio.to(device)
            predictions = model(audio).cpu().numpy()

            all_predictions.append(predictions)
            all_targets.append(params.numpy())

    predictions = np.concatenate(all_predictions, axis=0)
    targets = np.concatenate(all_targets, axis=0)

    # Metrics
    rmse = np.sqrt(mean_squared_error(targets, predictions))
    mae = mean_absolute_error(targets, predictions)

    # Per-parameter Pearson correlation
    correlations = []
    for i in range(targets.shape[1]):
        corr, _ = pearsonr(targets[:, i].flatten(), predictions[:, i].flatten())
        correlations.append(corr)

    print(f"RMSE: {rmse:.4f}")
    print(f"MAE: {mae:.4f}")
    print(f"Mean Pearson Correlation: {np.mean(correlations):.4f}")

    return {
        'rmse': rmse,
        'mae': mae,
        'correlations': correlations
    }
```

**ëª©í‘œ ì„±ëŠ¥ (Milestone M3):**
- **RMSE**: < 0.10 (normalized parameters)
- **Pearson Correlation**: > 0.70 per parameter
- **MAE**: < 0.08

**ì´ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ë©´ ë³¸ ì—°êµ¬ì˜ í•µì‹¬ ê³¼ì œê°€ ì™„ë£Œë©ë‹ˆë‹¤!**

---

## 4. í–¥í›„ í™•ì¥ ê³„íš (Phase 3: Digital Twin)

**ì£¼ì˜: ì´ ì„¹ì…˜ì€ Phase 1-2ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œëœ í›„ì— ì§„í–‰í•˜ëŠ” í™•ì¥ ì—°êµ¬ì…ë‹ˆë‹¤.**
**í˜„ì¬ëŠ” Phase 1-2ì— ì§‘ì¤‘í•´ì£¼ì„¸ìš”!**

### Phase 3: ë””ì§€í„¸ íŠ¸ìœˆ êµ¬ì¶• ë° ìŒí–¥ ì‹œë®¬ë ˆì´ì…˜ (ëª©í‘œ 2)

#### ëª©í‘œ
ì¶”ë¡ ëœ íŒŒë¼ë¯¸í„°ë¡œ 3D Meshë¥¼ ë³€í˜•í•˜ê³ , ì´ë¥¼ í†µí•´ ì†Œë¦¬ë¥¼ í•©ì„±

```
Articulatory Parameters (10-dim)
   â†“
3D Vocal Tract Reconstruction
   â†“
Physics-based Acoustic Simulation
   â†“
Synthesized Audio Waveform
```

---

#### 3.3.1. 3D Mesh ìƒì„±

**ë°©ë²•ë¡  1: MRI ìŠ¬ë¼ì´ìŠ¤ ì ì¸µ (Stacking)**
```python
# src/simulation/mesh_generator.py

import numpy as np
from scipy.interpolate import interp1d
import trimesh

def stack_mri_slices_to_3d(
    segmentation_masks: np.ndarray,  # (num_slices, H, W)
    slice_thickness: float = 2.0     # mm
) -> trimesh.Trimesh:
    """
    2D MRI ë¶„í•  ë§ˆìŠ¤í¬ë¥¼ ì ì¸µí•˜ì—¬ 3D Mesh ìƒì„±
    """
    # Extract contours from each slice
    contours = []
    for i, mask in enumerate(segmentation_masks):
        contour = extract_contour(mask)  # Returns (N, 2) points
        z_coord = i * slice_thickness
        # Add z-coordinate
        contour_3d = np.column_stack([contour, np.full(len(contour), z_coord)])
        contours.append(contour_3d)

    # Connect contours to form mesh
    vertices, faces = connect_contours(contours)
    mesh = trimesh.Trimesh(vertices=vertices, faces=faces)

    return mesh
```

**ë°©ë²•ë¡  2: íŒŒë¼ë©”íŠ¸ë¦­ ëª¨ë¸ ë³€í˜•**
```python
def deform_vocal_tract_template(
    template_mesh: trimesh.Trimesh,
    articulatory_params: np.ndarray  # (10,)
) -> trimesh.Trimesh:
    """
    Template meshë¥¼ articulatory parametersë¡œ ë³€í˜•
    """
    # Parameters control specific deformation modes
    # e.g., params[0] -> tongue height
    #       params[1] -> tongue frontness
    #       params[2] -> jaw opening

    deformed_vertices = template_mesh.vertices.copy()

    # Apply deformations (simplified example)
    for i, param in enumerate(articulatory_params):
        deformation_vector = deformation_basis[i]  # Pre-computed basis
        deformed_vertices += param * deformation_vector

    deformed_mesh = trimesh.Trimesh(
        vertices=deformed_vertices,
        faces=template_mesh.faces
    )

    return deformed_mesh
```

---

#### 3.3.2. ìŒí–¥ ì‹œë®¬ë ˆì´ì…˜

**ë°©ë²•ë¡  1: VocalTractLab ì—°ë™**

[VocalTractLab](http://www.vocaltractlab.de/)ì€ ë°œìŒ ê¸°ê´€ì˜ ë¬¼ë¦¬ ê¸°ë°˜ ì‹œë®¬ë ˆì´í„°ì…ë‹ˆë‹¤.

```python
# src/simulation/acoustic_synthesizer.py

import subprocess
import numpy as np

class VocalTractLabSynthesizer:
    def __init__(self, vtl_path: str = "/usr/local/bin/VocalTractLab"):
        self.vtl_path = vtl_path

    def synthesize(
        self,
        vocal_tract_params: np.ndarray,  # (time, num_params)
        output_path: str
    ) -> np.ndarray:
        """
        VocalTractLabì„ ì‚¬ìš©í•˜ì—¬ ìŒí–¥ í•©ì„±
        """
        # Write parameters to VTL format
        param_file = "temp_params.txt"
        self.write_vtl_params(vocal_tract_params, param_file)

        # Call VocalTractLab
        subprocess.run([
            self.vtl_path,
            "--synthesize",
            "--input", param_file,
            "--output", output_path
        ])

        # Load synthesized audio
        audio, sr = librosa.load(output_path, sr=16000)
        return audio
```

**ë°©ë²•ë¡  2: FEM (Finite Element Method) ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜**

```python
# src/simulation/fem_acoustic.py

def simulate_acoustic_fem(
    mesh: trimesh.Trimesh,
    glottal_source: np.ndarray,  # (time,) - ì„±ëŒ€ íŒŒí˜•
    sampling_rate: int = 16000
) -> np.ndarray:
    """
    ìœ í•œ ìš”ì†Œë²•ìœ¼ë¡œ ì„±ë„ ë‚´ë¶€ ìŒíŒŒ ì „íŒŒ ì‹œë®¬ë ˆì´ì…˜
    """
    # 1. Meshë¥¼ FEM solver í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    # 2. ê²½ê³„ ì¡°ê±´ ì„¤ì • (ì…ìˆ  = ë°©ì‚¬ ê²½ê³„)
    # 3. íŒŒë™ ë°©ì •ì‹ ìˆ˜ì¹˜ í•´ì„
    # 4. ì…ìˆ  ìœ„ì¹˜ì—ì„œ ìŒì•• ê¸°ë¡ -> ì¶œë ¥ ì˜¤ë””ì˜¤

    # Simplified pseudo-code
    solver = AcousticFEMSolver(mesh)
    output_audio = solver.solve(glottal_source, sampling_rate)

    return output_audio
```

**ë°©ë²•ë¡  3: Neural Vocoder (End-to-End í•™ìŠµ)**

ë¬¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ ëŒ€ì‹  ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ë¡œ ì§ì ‘ í•™ìŠµ:

```python
class ArticulatoryNeuralVocoder(nn.Module):
    """
    Articulatory parameters -> Audio waveform
    """
    def __init__(self, param_dim: int = 10):
        super().__init__()

        # Upsample parameters to audio rate
        self.upsampler = nn.ConvTranspose1d(param_dim, 256, kernel_size=400, stride=200)

        # WaveNet-style generator
        self.wavenet = WaveNetDecoder(channels=256)

    def forward(self, params):
        # params: (batch, time, param_dim)
        params = params.transpose(1, 2)  # (batch, param_dim, time)

        upsampled = self.upsampler(params)  # (batch, 256, audio_time)
        audio = self.wavenet(upsampled)      # (batch, 1, audio_time)

        return audio
```

---

#### 3.3.3. End-to-End íŒŒì´í”„ë¼ì¸

**ì „ì²´ ì‹œìŠ¤í…œ í†µí•©:**
```python
# src/pipeline/end_to_end.py

class DigitalTwinPipeline:
    def __init__(self):
        # Phase 2: Audio -> Parameters
        self.param_predictor = BiLSTMArticulationPredictor()
        self.param_predictor.load_state_dict(torch.load("models/phase2_best.pth"))

        # Phase 3: Parameters -> Audio
        self.synthesizer = VocalTractLabSynthesizer()

    def synthesize_from_audio(self, input_audio_path: str, output_audio_path: str):
        """
        ì…ë ¥ ì˜¤ë””ì˜¤ë¥¼ ë°›ì•„ ë””ì§€í„¸ íŠ¸ìœˆìœ¼ë¡œ ì¬í•©ì„±
        """
        # 1. Extract features
        mfcc = extract_mfcc(input_audio_path)

        # 2. Predict articulatory parameters
        with torch.no_grad():
            params = self.param_predictor(torch.FloatTensor(mfcc).unsqueeze(0))
            params = params.squeeze(0).numpy()

        # 3. Synthesize audio
        synthesized_audio = self.synthesizer.synthesize(params, output_audio_path)

        print(f"Synthesis complete: {output_audio_path}")
        return synthesized_audio

# Usage
pipeline = DigitalTwinPipeline()
pipeline.synthesize_from_audio("input.wav", "output_synthesized.wav")
```

**Phase 3 ì²´í¬ë¦¬ìŠ¤íŠ¸ (ì°©ìˆ˜ ì „ í™•ì¸):**
- [ ] Phase 2 ëª¨ë¸ì´ ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„± (RMSE < 0.10, PCC > 0.70)
- [ ] Phase 2 ëª¨ë¸ì´ ë‹¤ì–‘í•œ í™”ìì—ê²Œ ì¼ë°˜í™”ë¨ì„ ê²€ì¦
- [ ] ë…¼ë¬¸ ì´ˆì•ˆ ì‘ì„± ì™„ë£Œ (Phase 1-2 ê²°ê³¼)
- [ ] í”„ë¡œì íŠ¸ ë¦¬ë”ì˜ Phase 3 ì°©ìˆ˜ ìŠ¹ì¸

---

## 5. ì„ í–‰ ì—°êµ¬ ë¶„ì„

### 4.1. ë…¼ë¬¸ ë¦¬ë·° í”„ë¡œì„¸ìŠ¤

ëª¨ë“  íŒ€ì›ì€ í• ë‹¹ëœ ë…¼ë¬¸ì„ ì½ê³  ë‹¤ìŒ í…œí”Œë¦¿ìœ¼ë¡œ ì •ë¦¬:

```markdown
## Paper Review: [ë…¼ë¬¸ ì œëª©]

**Reviewer**: [ì´ë¦„]
**Date**: YYYY-MM-DD
**Paper Link**: [URL or DOI]

### 1. Summary (3-5 ë¬¸ì¥)
[ë…¼ë¬¸ì˜ í•µì‹¬ ë‚´ìš© ìš”ì•½]

### 2. Key Contributions
- [ê¸°ì—¬ 1]
- [ê¸°ì—¬ 2]

### 3. Methodology
[ì‚¬ìš©í•œ ë°©ë²•ë¡  ìƒì„¸ ì„¤ëª…]

### 4. Results
- **Dataset**: [ì‚¬ìš©í•œ ë°ì´í„°ì…‹]
- **Metrics**: [í‰ê°€ ì§€í‘œ ë° ê²°ê³¼]

### 5. Relevance to Our Project
[ìš°ë¦¬ ì—°êµ¬ì— ì–´ë–»ê²Œ ì ìš©í•  ìˆ˜ ìˆëŠ”ì§€]

### 6. Code/Resources
- Code: [GitHub link if available]
- Dataset: [Download link]

### 7. Limitations
[ë…¼ë¬¸ì˜ í•œê³„ì ]

### 8. Future Work / Our Improvements
[ìš°ë¦¬ê°€ ê°œì„ í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„]
```

ì €ì¥ ìœ„ì¹˜: `docs/literature_review/YYYYMMDD_paper_title.md`

---

### 4.2. í•„ìˆ˜ ë¦¬ë·° ëŒ€ìƒ ë…¼ë¬¸

#### 5.2.1. ë°ì´í„°ì…‹ & ì „ì²˜ë¦¬ (Phase 1 - ìš°ì„  ë¦¬ë·° â­)

1. **"A Real-Time MRI Database for Speech Production"** (Narayanan et al.)
   - USC-TIMIT ë°ì´í„°ì…‹ì˜ ì›ë³¸ ë…¼ë¬¸
   - ë‹´ë‹¹ì: [ì´ë¦„]
   - ë§ˆê°: [ë‚ ì§œ]
   - **ìš°ì„ ìˆœìœ„: ë†’ìŒ**

2. **"Automatic Segmentation of the Vocal Tract from Real-Time MRI"**
   - MRI ë¶„í•  ë°©ë²•ë¡ 
   - ë‹´ë‹¹ì: [ì´ë¦„]
   - **ìš°ì„ ìˆœìœ„: ë†’ìŒ**

#### 5.2.2. Articulatory Inversion (Phase 2 - í•„ìˆ˜ ë¦¬ë·° â­â­)

3. **"Deep Learning for Acoustic-to-Articulatory Inversion"** (Ribeiro et al., 2019)
   - LSTM ê¸°ë°˜ ìŒí–¥-ì¡°ìŒ ë³€í™˜
   - ë‹´ë‹¹ì: [ì´ë¦„]
   - **ìš°ì„ ìˆœìœ„: ìµœê³  (ë³¸ ì—°êµ¬ì˜ í•µì‹¬)**

4. **"Transformer-based Acoustic-to-Articulatory Speech Inversion"**
   - ìµœì‹  Transformer ì‘ìš©
   - ë‹´ë‹¹ì: [ì´ë¦„]
   - **ìš°ì„ ìˆœìœ„: ìµœê³ **

5. **"Learning Acoustic-Articulatory Mapping with LSTM Networks"**
   - ì‹œê³„ì—´ ë§¤í•‘ í•™ìŠµ ë°©ë²•ë¡ 
   - ë‹´ë‹¹ì: [ì´ë¦„]
   - **ìš°ì„ ìˆœìœ„: ë†’ìŒ**

#### 5.2.3. ìŒí–¥ í•©ì„± (Phase 3 - í–¥í›„ ì°¸ê³ ìš©)

6. **"VocalTractLab: An Articulatory Speech Synthesizer"** (Birkholz, 2013)
   - ë¬¼ë¦¬ ê¸°ë°˜ í•©ì„±ì˜ í‘œì¤€
   - ë‹´ë‹¹ì: [ì´ë¦„]
   - **ìš°ì„ ìˆœìœ„: ë‚®ìŒ (Phase 3 ì°©ìˆ˜ ì‹œ ë¦¬ë·°)**

7. **"Neural Vocoding with Articulatory Features"**
   - ë‰´ëŸ´ ë³´ì½”ë” ì ‘ê·¼
   - ë‹´ë‹¹ì: [ì´ë¦„]
   - **ìš°ì„ ìˆœìœ„: ë‚®ìŒ (Phase 3 ì°©ìˆ˜ ì‹œ ë¦¬ë·°)**

---

### 4.3. í‚¤ì›Œë“œë³„ ê²€ìƒ‰ ì „ëµ

ì •ê¸°ì ìœ¼ë¡œ ë‹¤ìŒ í‚¤ì›Œë“œë¡œ ìµœì‹  ë…¼ë¬¸ ê²€ìƒ‰:

- **arXiv / Google Scholar ê²€ìƒ‰ì–´ (ìš°ì„ ìˆœìœ„ë³„):**
  - **Phase 1-2 (í•„ìˆ˜):**
    - "acoustic to articulatory inversion"
    - "speech to vocal tract parameters"
    - "articulatory feature extraction from audio"
    - "vocal tract MRI segmentation"
    - "real-time MRI speech"
  - **Phase 3 (í–¥í›„ ì°¸ê³ ):**
    - "articulatory speech synthesis"
    - "digital twin vocal tract"
    - "physical speech synthesis"

- **ì£¼ìš” í•™íšŒ:**
  - INTERSPEECH
  - ICASSP
  - IEEE Transactions on Audio, Speech, and Language Processing

---

## 6. í‰ê°€ ì§€í‘œ

### 6.1. Phase 1 í‰ê°€ (ë°ì´í„° ì „ì²˜ë¦¬)

| ì§€í‘œ | ëª©í‘œ | ì¸¡ì • ë°©ë²• |
|------|------|----------|
| Segmentation Dice Coefficient | > 0.85 | Ground truthì™€ ë¹„êµ |
| Segmentation IoU | > 0.80 | Ground truthì™€ ë¹„êµ |
| Parameter Reconstruction Error | < 5% | Autoencoder reconstruction loss |

---

### 6.2. Phase 2 í‰ê°€ (Audio-to-Parameter) - í•µì‹¬ í‰ê°€ ì§€í‘œ â­

**ì´ê²ƒì´ ë³¸ ì—°êµ¬ì˜ ì„±ê³µ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” í•µì‹¬ ì§€í‘œì…ë‹ˆë‹¤!**

#### 6.2.1. ê¸°í•˜í•™ì  ì •í™•ë„ (Geometric Accuracy)

```python
# src/evaluation/metrics.py

def compute_rmse(predictions, targets):
    """Root Mean Squared Error"""
    return np.sqrt(np.mean((predictions - targets) ** 2))

def compute_mae(predictions, targets):
    """Mean Absolute Error"""
    return np.mean(np.abs(predictions - targets))

def compute_pearson_correlation(predictions, targets):
    """Pearson Correlation Coefficient per parameter"""
    from scipy.stats import pearsonr

    correlations = []
    for i in range(predictions.shape[1]):  # For each parameter
        corr, p_value = pearsonr(
            predictions[:, i].flatten(),
            targets[:, i].flatten()
        )
        correlations.append(corr)

    return np.array(correlations)
```

**ëª©í‘œ ì„±ëŠ¥ (Milestone M3):**
- **RMSE**: < 0.10 (normalized) - **í•„ìˆ˜ ë‹¬ì„±**
- **MAE**: < 0.08 - **í•„ìˆ˜ ë‹¬ì„±**
- **Pearson Correlation**: > 0.70 (per parameter) - **í•„ìˆ˜ ë‹¬ì„±**

**ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ (Milestone M2):**
- RMSE < 0.15
- Pearson Correlation > 0.50

**ì„±ëŠ¥ í‰ê°€ ì£¼ê¸°:**
- ë§¤ epochë§ˆë‹¤ validation set í‰ê°€
- ë§¤ì£¼ test setìœ¼ë¡œ ì „ì²´ í‰ê°€ ë° ê²°ê³¼ ê¸°ë¡

---

### 6.3. Phase 3 í‰ê°€ (ìŒí–¥ í•©ì„±) - í–¥í›„ ì°¸ê³ ìš©

**ì£¼ì˜: Phase 1-2 ì™„ë£Œ í›„ì—ë§Œ ì ìš©ë˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤.**

#### 6.3.1. ìŒí–¥í•™ì  ì •í™•ë„ (Acoustic Accuracy)

```python
def compute_lsd(original_audio, synthesized_audio, sr=16000):
    """
    Log-Spectral Distance (LSD)
    ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (< 1.0 dBê°€ ëª©í‘œ)
    """
    from scipy.signal import spectrogram

    # Compute spectrograms
    _, _, S_orig = spectrogram(original_audio, fs=sr)
    _, _, S_synth = spectrogram(synthesized_audio, fs=sr)

    # Log-spectral distance
    lsd = np.mean(np.sqrt(np.mean((10 * np.log10(S_orig + 1e-10) -
                                     10 * np.log10(S_synth + 1e-10)) ** 2, axis=0)))
    return lsd

def compute_pesq(original_audio, synthesized_audio, sr=16000):
    """
    PESQ (Perceptual Evaluation of Speech Quality)
    ë²”ìœ„: -0.5 ~ 4.5 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
    """
    from pesq import pesq
    score = pesq(sr, original_audio, synthesized_audio, 'wb')  # wideband
    return score

def compute_stoi(original_audio, synthesized_audio, sr=16000):
    """
    STOI (Short-Time Objective Intelligibility)
    ë²”ìœ„: 0 ~ 1 (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
    """
    from pystoi import stoi
    score = stoi(original_audio, synthesized_audio, sr, extended=False)
    return score
```

**ëª©í‘œ ì„±ëŠ¥:**
- **LSD**: < 1.5 dB
- **PESQ**: > 3.0
- **STOI**: > 0.75

---

### 6.4. ì‹¤í—˜ ê²°ê³¼ ê¸°ë¡ í˜•ì‹

ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ JSONì— ì €ì¥:

```json
{
  "experiment_id": "EXP-20251125-01",
  "date": "2025-11-25",
  "phase": 2,
  "milestone": "M2",  // M1, M2, M3, M4
  "model": "BiLSTM_Articulation_Predictor",
  "config": {
    "hidden_dim": 256,
    "num_layers": 3,
    "learning_rate": 0.001,
    "batch_size": 32
  },
  "dataset": {
    "train_size": 8000,
    "val_size": 1000,
    "test_size": 1000
  },
  "metrics": {
    "train_loss": 0.0523,
    "val_loss": 0.0687,
    "test_rmse": 0.0891,
    "test_mae": 0.0712,
    "test_pearson_correlation": [0.78, 0.82, 0.75, 0.79, 0.81, 0.77, 0.80, 0.76, 0.83, 0.79]
  },
  "notes": "Added smoothness loss with alpha=0.1"
}
```

---

## 7. ì´ˆê¸° ì„¤ì • ë° ë‹¹ë©´ ê³¼ì œ

**í˜„ì¬ ìš°ì„ ìˆœìœ„: Phase 1 ì™„ë£Œ â†’ Milestone M1 ë‹¬ì„±**

### 7.1. í™˜ê²½ ì„¤ì • (Environment Setup)

#### 6.1.1. Python ê°€ìƒí™˜ê²½ ìƒì„±

```bash
# Create virtual environment
python3 -m venv venv_sullivan

# Activate
source venv_sullivan/bin/activate  # Linux/Mac
# venv_sullivan\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt
```

#### 6.1.2. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ (requirements.txt)

```txt
# Deep Learning
torch>=2.0.0
torchvision>=0.15.0
lightning>=2.0.0

# Data Processing
numpy>=1.24.0
scipy>=1.10.0
scikit-learn>=1.3.0
pandas>=2.0.0

# Audio Processing
librosa>=0.10.0
soundfile>=0.12.0
pydub>=0.25.0

# Image/Video Processing
opencv-python>=4.8.0
scikit-image>=0.21.0
pillow>=10.0.0

# Medical Image Processing
nibabel>=5.0.0  # NIfTI format
pydicom>=2.4.0  # DICOM format

# 3D Processing
trimesh>=3.23.0
open3d>=0.17.0

# Evaluation
pesq>=0.0.4
pystoi>=0.3.3

# Utilities
tqdm>=4.65.0
matplotlib>=3.7.0
seaborn>=0.12.0
pyyaml>=6.0
tensorboard>=2.13.0

# Notebook
jupyter>=1.0.0
ipywidgets>=8.0.0
```

---

### 7.2. ë°ì´í„°ì…‹ ì´ˆê¸° ì„¤ì •

**ëª©í‘œ: Milestone M1 ë‹¬ì„±ì„ ìœ„í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•**

#### Task 1: ë°ì´í„° ì••ì¶• í•´ì œ ë° êµ¬ì¡° íŒŒì•… (Phase 1 - Step 1)

```bash
# ì••ì¶• í•´ì œ
cd /home/midori/Develop/Project_Sullivan
unzip usc_speech_mri-master.zip -d data/raw/

# êµ¬ì¡° í™•ì¸
tree data/raw/usc_speech_mri-master -L 2
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] ì••ì¶• í•´ì œ ì™„ë£Œ
- [ ] ë””ë ‰í† ë¦¬ êµ¬ì¡° ë¬¸ì„œí™”
- [ ] ìƒ˜í”Œ ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸
- [ ] ë©”íƒ€ë°ì´í„° íŒŒì¼ í™•ì¸

---

#### Task 2: íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA) (Phase 1 - Step 1)

**ëª©í‘œ: ë°ì´í„°ì…‹ íŠ¹ì„± íŒŒì•… ë° ì „ì²˜ë¦¬ ì „ëµ ìˆ˜ë¦½**

**Jupyter Notebook ìƒì„±:** `notebooks/01_EDA.ipynb`

```python
# EDA í•„ìˆ˜ í™•ì¸ ì‚¬í•­

import os
import numpy as np
import matplotlib.pyplot as plt
from glob import glob

# 1. ë°ì´í„° íŒŒì¼ ìˆ˜ í™•ì¸
mri_files = glob("data/raw/usc_speech_mri-master/**/*.png", recursive=True)
audio_files = glob("data/raw/usc_speech_mri-master/**/*.wav", recursive=True)

print(f"Total MRI frames: {len(mri_files)}")
print(f"Total audio files: {len(audio_files)}")

# 2. ìƒ˜í”Œ MRI í”„ë ˆì„ ë¡œë“œ
sample_mri = plt.imread(mri_files[0])
print(f"MRI frame shape: {sample_mri.shape}")
print(f"MRI dtype: {sample_mri.dtype}")
print(f"MRI value range: [{sample_mri.min()}, {sample_mri.max()}]")

# 3. ìƒ˜í”Œ ì˜¤ë””ì˜¤ ë¡œë“œ
import librosa
audio, sr = librosa.load(audio_files[0], sr=None)
print(f"Audio length: {len(audio)} samples")
print(f"Sample rate: {sr} Hz")
print(f"Duration: {len(audio) / sr:.2f} seconds")

# 4. ì‹œê°í™”
fig, axes = plt.subplots(1, 2, figsize=(12, 4))
axes[0].imshow(sample_mri, cmap='gray')
axes[0].set_title("Sample MRI Frame")
axes[1].plot(audio[:sr])  # First second
axes[1].set_title("Audio Waveform (1 sec)")
plt.tight_layout()
plt.savefig("results/eda_sample.png")
```

**ì¶œë ¥ ë¬¸ì„œ:** `docs/data_statistics.md`

---

#### Task 3: Baseline ëª¨ë¸ êµ¬ì¶• (Phase 2 - Milestone M2)

**ëª©í‘œ:** ê°„ë‹¨í•œ DNNìœ¼ë¡œ Audio â†’ Articulatory Parameters ì˜ˆì¸¡ ë² ì´ìŠ¤ë¼ì¸ í™•ë¦½
**ì„±ê³µ ê¸°ì¤€:** RMSE < 0.15, PCC > 0.50

```python
# src/baseline/simple_predictor.py

import torch
import torch.nn as nn

class SimpleBaselinePredictor(nn.Module):
    """
    ìµœì†Œ ê¸°ëŠ¥ ë² ì´ìŠ¤ë¼ì¸
    Input: MFCC (13,)
    Output: Flattened MRI pixels or PCA components
    """
    def __init__(self, input_dim=13, output_dim=100):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, output_dim)
        )

    def forward(self, x):
        return self.fc(x)

# Training
model = SimpleBaselinePredictor()
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# ... training loop ...
```

**ì„±ê³µ ê¸°ì¤€:**
- Lossê°€ ìˆ˜ë ´í•˜ëŠ”ì§€ í™•ì¸
- Validation RMSE < 0.15
- ì´ ë² ì´ìŠ¤ë¼ì¸ì´ í–¥í›„ ê°œì„ ì˜ ê¸°ì¤€ì ì´ ë¨

---

### 7.3. ì´ˆê¸° ì‘ì—… ì²´í¬ë¦¬ìŠ¤íŠ¸ (Milestone M1 ë‹¬ì„± ê²½ë¡œ)

**Phase 1 ì‘ì—… ìˆœì„œ:**
- [ ] Task 1: ë°ì´í„° ì••ì¶• í•´ì œ ë° êµ¬ì¡° íŒŒì•… (1ì£¼)
- [ ] Task 2: EDA ìˆ˜í–‰ ë° ë°ì´í„° íŠ¹ì„± íŒŒì•… (1ì£¼)
- [ ] Step 2: MRI/Audio ì „ì²˜ë¦¬ ë° ì •ë ¬ (1-2ì£¼)
- [ ] Step 3: MRI ROI ë¶„í•  ëª¨ë¸ í•™ìŠµ ë˜ëŠ” ì ìš© (2-3ì£¼)
- [ ] Step 4: ë°œìŒ ê¸°ê´€ íŒŒë¼ë¯¸í„° ì¶”ì¶œ (PCA/Autoencoder) (1-2ì£¼)
- [ ] ë°ì´í„°ì…‹ ë¶„í•  (Train/Val/Test) ë° ì €ì¥

**Phase 2 ì‘ì—… ìˆœì„œ (M1 ì™„ë£Œ í›„):**
- [ ] Task 3: ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ êµ¬ì¶• (Milestone M2) (2-3ì£¼)
- [ ] ëª¨ë¸ ê°œì„  ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (4-6ì£¼)
- [ ] ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„± (Milestone M3)

---

### 7.4. íŒ€ ì—­í•  ë¶„ë‹´ (ê¶Œì¥)

**Phase 1-2 ì§‘ì¤‘ ì—­í• :**

| ì—­í•  | ë‹´ë‹¹ì | ì±…ì„ ë²”ìœ„ | í˜„ì¬ ìš°ì„ ìˆœìœ„ |
|------|--------|----------|--------------|
| **Project Lead** | [ì´ë¦„] | ì „ì²´ ì§„í–‰ ê´€ë¦¬, ë§ˆì¼ìŠ¤í†¤ ì¶”ì  | â­â­â­ |
| **Data Engineer** | [ì´ë¦„] | Phase 1 ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (M1 ë‹¬ì„±) | â­â­â­ |
| **ML Engineer 1** | [ì´ë¦„] | Phase 2 ëª¨ë¸ ê°œë°œ (M2, M3 ë‹¬ì„±) | â­â­â­ |
| **ML Engineer 2** | [ì´ë¦„] | ëª¨ë¸ ìµœì í™”, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ | â­â­ |
| **Research Analyst** | [ì´ë¦„] | ë…¼ë¬¸ ë¦¬ë·°, í‰ê°€ ì§€í‘œ ë¶„ì„ | â­â­ |

**Phase 3 (í–¥í›„):**
| **Simulation Engineer** | [ì´ë¦„] | Phase 3 3D ë° ìŒí–¥ ì‹œë®¬ë ˆì´ì…˜ | (Phase 1-2 ì™„ë£Œ í›„ ì°¸ì—¬) |

---

### 7.5. ì£¼ê°„ íšŒì˜ í”„ë¡œí† ì½œ

**ë§¤ì£¼ [ìš”ì¼] [ì‹œê°„]ì— ì§„í–‰**

#### íšŒì˜ ì „ ì¤€ë¹„ì‚¬í•­
- ê°ì ì‘ì—… ë¡œê·¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ
- **í˜„ì¬ ë§ˆì¼ìŠ¤í†¤ ì§„ì²™ë„** ì²´í¬
- ì£¼ìš” ê²°ê³¼ ìŠ¬ë¼ì´ë“œ 1-2ì¥ ì¤€ë¹„

#### íšŒì˜ ì•ˆê±´ (30-60ë¶„)
1. **ë§ˆì¼ìŠ¤í†¤ ì§„ì²™ë„ í™•ì¸** (5ë¶„)
   - í˜„ì¬ ë§ˆì¼ìŠ¤í†¤: M?
   - ëª©í‘œ ë‹¬ì„±ë¥ : ?%

2. ì§€ë‚œ ì£¼ ì‘ì—… ë¦¬ë·° (ê° 10ë¶„)
   - ì™„ë£Œí•œ ì‘ì—…
   - ì£¼ìš” ê²°ê³¼ ë° ì§€í‘œ (íŠ¹íˆ RMSE, PCC)
   - ë°œìƒí•œ ë¬¸ì œ

3. ì´ìŠˆ í† ë¡  (15ë¶„)
   - ë¸”ë¡œí‚¹ ì´ìŠˆ í•´ê²°
   - ê¸°ìˆ ì  ë‚œì œ ë¸Œë ˆì¸ìŠ¤í† ë°

4. ë‹¤ìŒ ì£¼ ê³„íš (10ë¶„)
   - ì‘ì—… í• ë‹¹
   - ë§ˆê° ê¸°í•œ ì„¤ì •
   - **ë‹¤ìŒ ë§ˆì¼ìŠ¤í†¤ ë‹¬ì„± ì „ëµ**

#### íšŒì˜ë¡ ì‘ì„±
- ë‹´ë‹¹: ìˆœë²ˆì œ
- ì €ì¥ ìœ„ì¹˜: `docs/meeting_notes/YYYYMMDD_meeting.md`

---

## 8. ë¶€ë¡

### 8.1. ìš©ì–´ ì‚¬ì „ (Glossary)

| ìš©ì–´ | ì„¤ëª… |
|------|------|
| **Articulatory Parameters** | ë°œìŒ ê¸°ê´€(í˜€, í„±, ì…ìˆ  ë“±)ì˜ ìœ„ì¹˜ì™€ í˜•íƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŒŒë¼ë¯¸í„° |
| **Vocal Tract** | ì„±ë„, ì„±ëŒ€ì—ì„œ ì…ìˆ ê¹Œì§€ì˜ ê³µê¸° í†µë¡œ |
| **rtMRI** | Real-time MRI, ì‹¤ì‹œê°„ ìê¸°ê³µëª…ì˜ìƒ |
| **MFCC** | Mel-Frequency Cepstral Coefficients, ì˜¤ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ ë°©ë²• |
| **Digital Twin** | ì‹¤ì œ ë¬¼ë¦¬ ì‹œìŠ¤í…œì„ ë””ì§€í„¸ë¡œ ì¬í˜„í•œ ëª¨ë¸ |
| **LSD** | Log-Spectral Distance, ìŠ¤í™íŠ¸ëŸ¼ ê±°ë¦¬ ì§€í‘œ |
| **PESQ** | Perceptual Evaluation of Speech Quality, ìŒì§ˆ í‰ê°€ ì§€í‘œ |
| **IoU** | Intersection over Union, ë¶„í•  ì •í™•ë„ ì§€í‘œ |

---

| **Acoustic-to-Articulatory Inversion** | ìŒì„± ì‹ í˜¸ë¡œë¶€í„° ë°œìŒ ê¸°ê´€ íŒŒë¼ë¯¸í„°ë¥¼ ì—­ì¶”ë¡ í•˜ëŠ” ê¸°ìˆ  (ë³¸ ì—°êµ¬ì˜ í•µì‹¬) |

### 8.2. íŠ¸ëŸ¬ë¸”ìŠˆíŒ… (Troubleshooting)

#### Issue 1: MRI ë°ì´í„° ë¡œë”© ì‹¤íŒ¨
**ì¦ìƒ:** `FileNotFoundError` ë˜ëŠ” ì´ë¯¸ì§€ ë¡œë”© ì—ëŸ¬

**í•´ê²°ì±…:**
```python
# íŒŒì¼ ê²½ë¡œ í™•ì¸
import os
assert os.path.exists(mri_path), f"File not found: {mri_path}"

# ì´ë¯¸ì§€ í¬ë§· í™•ì¸ (PNG, DICOM, NIfTI ë“±)
from PIL import Image
img = Image.open(mri_path)
```

---

#### Issue 2: CUDA Out of Memory
**ì¦ìƒ:** `RuntimeError: CUDA out of memory`

**í•´ê²°ì±…:**
```python
# 1. Batch size ì¤„ì´ê¸°
batch_size = 16  # 32ì—ì„œ 16ìœ¼ë¡œ

# 2. Gradient accumulation ì‚¬ìš©
accumulation_steps = 4
for i, (audio, params) in enumerate(dataloader):
    loss = model(audio, params)
    loss = loss / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# 3. Mixed precision training
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

with autocast():
    loss = model(audio, params)
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

---

#### Issue 3: ëª¨ë¸ì´ ìˆ˜ë ´í•˜ì§€ ì•ŠìŒ
**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] Learning rateê°€ ì ì ˆí•œê°€? (0.001 ~ 0.0001 ì‹œë„)
- [ ] ë°ì´í„° ì •ê·œí™”ê°€ ë˜ì–´ìˆëŠ”ê°€? (Mean 0, Std 1)
- [ ] Lossê°€ NaNì´ ë˜ì§€ ì•ŠëŠ”ê°€? (Gradient clipping ì ìš©)
- [ ] ë² ì´ìŠ¤ë¼ì¸ë³´ë‹¤ ë³µì¡í•œ ëª¨ë¸ì¸ê°€? (ë‹¨ìˆœ ëª¨ë¸ë¶€í„° ì‹œì‘)

```python
# Gradient clipping
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

---

### 8.3. ìœ ìš©í•œ ë¦¬ì†ŒìŠ¤

#### 7.3.1. ê³µê°œ ë°ì´í„°ì…‹
- **USC-TIMIT**: https://sail.usc.edu/span/usc-timit/
- **MNGU0 Articulatory Corpus**: http://www.mngu0.org/
- **EMA Database**: http://www.cs.toronto.edu/~hinton/ema/

#### 7.3.2. ì˜¤í”ˆì†ŒìŠ¤ ë„êµ¬
- **VocalTractLab**: http://www.vocaltractlab.de/
- **Praat** (ìŒì„± ë¶„ì„): https://www.fon.hum.uva.nl/praat/
- **Audacity** (ì˜¤ë””ì˜¤ í¸ì§‘): https://www.audacityteam.org/

#### 7.3.3. í•™ìŠµ ìë£Œ
- **Speech Signal Processing (Course)**: https://www.coursera.org/learn/audio-signal-processing
- **Articulatory Phonetics**: http://www.phonetics.ucla.edu/

---

### 8.4. ë¼ì´ì„ ìŠ¤ ë° ìœ¤ë¦¬

#### ë°ì´í„° ì‚¬ìš© ê·œì •
- USC-TIMIT ë°ì´í„°ì…‹ì€ **ì—°êµ¬ ëª©ì ìœ¼ë¡œë§Œ** ì‚¬ìš© ê°€ëŠ¥
- ìƒì—…ì  ì‚¬ìš© ì‹œ ë³„ë„ ë¼ì´ì„ ìŠ¤ í•„ìš”
- ë…¼ë¬¸ ë°œí‘œ ì‹œ ë°ì´í„°ì…‹ ì¶œì²˜ ëª…ì‹œ í•„ìˆ˜

#### ì¸ìš© í˜•ì‹
```
Narayanan, S., Byrd, D., & Kaun, A. (1999).
"Speech production data for research and education."
Journal of the Acoustical Society of America.
```

---

### 8.5. ë²„ì „ ê´€ë¦¬

ë³¸ ë§¤ë‰´ì–¼ì€ ì—°êµ¬ ì§„í–‰ì— ë”°ë¼ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.

| Version | Date | Changes | Author |
|---------|------|---------|--------|
| 1.0 | 2025-11-25 | ì´ˆê¸° ë²„ì „ ì‘ì„± | [ì´ë¦„] |
| 1.1 | 2025-11-25 | ì—°êµ¬ ìš°ì„ ìˆœìœ„ ëª…í™•í™” (ëª©í‘œ 1 ì¤‘ì‹¬), ë§ˆì¼ìŠ¤í†¤ ì¶”ê°€ | [ì´ë¦„] |
| | | | |

---

## ğŸ“ ì—°ë½ì²˜ ë° ì§€ì›

**í”„ë¡œì íŠ¸ ë¦¬ë”:**
- Name: [ì´ë¦„]
- Email: [ì´ë©”ì¼]
- Slack: @username

**ê¸´ê¸‰ ì´ìŠˆ ë³´ê³ :**
GitHub Issues: https://github.com/[org]/Project_Sullivan/issues

**ì •ê¸° íšŒì˜:**
ë§¤ì£¼ [ìš”ì¼] [ì‹œê°„] @ [ì¥ì†Œ/Zoom]

---

## ğŸ¯ ì—°êµ¬ ëª©í‘œ ì¬í™•ì¸

### ëª©í‘œ 1 (í˜„ì¬ ì§‘ì¤‘) â­â­â­

> **"ìŒì„± ì‹ í˜¸ë§Œ ì…ë ¥í•˜ë©´, ë°œìŒ ê¸°ê´€ì˜ ìœ„ì¹˜ì™€ ì›€ì§ì„ì„ ë‚˜íƒ€ë‚´ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ì •í™•í•˜ê²Œ ì¶”ë¡ í•œë‹¤."**

**Input**: ìŒì„± íŒŒí˜• (Audio Waveform)
**Output**: ë°œìŒ ê¸°ê´€ íŒŒë¼ë¯¸í„° (Articulatory Parameters)
- í˜€ì˜ ìœ„ì¹˜ (ë†’ì´, ì „í›„ ìœ„ì¹˜)
- í„±ì˜ ê°œë°©ë„
- ì…ìˆ ì˜ ëª¨ì–‘ (ì›ìˆœì„±, ê°œë°©ë„)
- ì—°êµ¬ê°œ, ì¸ë‘ ë“±ì˜ ìƒíƒœ

**ì„±ê³µ ê¸°ì¤€**: RMSE < 0.10, Pearson Correlation > 0.70

---

### ëª©í‘œ 2 (í–¥í›„ í™•ì¥)

> **"ì¶”ë¡ ëœ íŒŒë¼ë¯¸í„°ë¡œ 3D ë°œìŒ ê¸°ê´€ì„ ì¬í˜„í•˜ê³ , ë¬¼ë¦¬ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ì†Œë¦¬ë¥¼ í•©ì„±í•œë‹¤."**

*ì´ ëª©í‘œëŠ” ëª©í‘œ 1ì´ ì„±ê³µì ìœ¼ë¡œ ë‹¬ì„±ëœ í›„ ì§„í–‰í•©ë‹ˆë‹¤.*

---

**ëª¨ë“  ì—°êµ¬ì›ì€ ë¨¼ì € ëª©í‘œ 1 ë‹¬ì„±ì— ì§‘ì¤‘í•©ë‹ˆë‹¤!**

**í˜„ì¬ ë§ˆì¼ìŠ¤í†¤**: M1 (Data Pipeline êµ¬ì¶•)
**ë‹¤ìŒ ë§ˆì¼ìŠ¤í†¤**: M2 (Baseline Model)
**ìµœì¢… ëª©í‘œ**: M3 (Core Goal Achievement)

---

*End of Researcher Manual v1.1*
